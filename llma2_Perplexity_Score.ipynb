{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iE8po0WkeO-p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /Users/benanakca/miniconda3/lib/python3.12/site-packages (0.21.4)\n",
            "Requirement already satisfied: filelock in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (2024.2.0)\n",
            "Requirement already satisfied: requests in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z5gBqkaXcOZ2"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yv0ybSKbcTSL"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets bitsandbytes einops wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zukURjQwgA0H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /Users/benanakca/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\") # Bu tokenı girebilirsin => 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipywidgets) (8.22.2)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipywidgets) (5.14.1)\n",
            "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
            "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: decorator in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
            "Requirement already satisfied: stack-data in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/benanakca/miniconda3/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install ipywidgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K60wAJ1UcVIs"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "177370ea5b0e47e4a47a0d255285d18b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# model_path = 'openlm-research/open_llama_3b'\n",
        "# model_path = 'openlm-research/open_llama_7b'\n",
        "model_path = 'meta-llama/Llama-2-7b-hf'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path, torch_dtype=torch.float16, device_map='auto')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/benanakca/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/accelerate/utils/modeling.py:1341: UserWarning: Current model requires 486542976 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d00995f4e3364b94925a03b892b9600c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.float16, device_map=\"auto\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "310906b00e6645309f8c91d1c3dc1c7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81fbc58bb7aa4e498b6709094aa03170",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "608259a362ad45eab7f5f6f1580563e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9041686c0ba84276afee758ee7a02e18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g0BuAk6bfVhN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ: What is the largest animal?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m generation_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      6\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device), max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(generation_output[\u001b[38;5;241m0\u001b[39m]))\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/accelerate/big_modeling.py:453\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 453\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
          ]
        }
      ],
      "source": [
        "device = \"cuda:0\"\n",
        "prompt = 'Q: What is the largest animal?\\nA:'\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "model.to(device)\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids.to(device), max_new_tokens=32\n",
        ")\n",
        "print(tokenizer.decode(generation_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-jdNhHvg1BJ",
        "outputId": "0b1e0d5e-b158-418e-da28-e63696ec3df0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.0286)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence1 = \"What is the largest animal\"\n",
        "sentence2 = \"Wall paper spcae\"\n",
        "input_ids = tokenizer.encode(sentence2, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, labels=input_ids)\n",
        "    log_likelihood = outputs[0].item()\n",
        "perplexity = torch.exp(torch.tensor(log_likelihood / input_ids.shape[-1]))\n",
        "perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vemzgTyyhpff",
        "outputId": "dc8c0d15-5cce-485d-d9e6-cd8c81b70958"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import nltk\n",
        "import ssl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# %%\n",
        "pd.options.display.max_columns = 2000\n",
        "pd.options.display.max_rows = 2000\n",
        "\n",
        "pd.options.display.max_colwidth = 2000\n",
        "\n",
        "# %%\n",
        "# Custom sentence tokenizer\n",
        "# def custom_sent_tokenize(text):\n",
        "#     # Replace common false positive splits\n",
        "#     text = text.replace('Or.', 'Or').replace('Therefore.', 'Therefore')\n",
        "#     text = text.replace('Dr.', 'Dr').replace('The St.', 'The St')\n",
        "#     text = text.replace('Very Truly Yours:', 'Very Truly Yours')\n",
        "#     text = text.replace('d.', 'd')\n",
        "\n",
        "#     # Split by sentence\n",
        "#     sentences = re.split('(?<=[.!?])\\s+', text)\n",
        "\n",
        "#     # Strip excess whitespace and filter out any non-sentences\n",
        "#     sentences = [sent.strip() for sent in sentences if len(sent.strip().split(' ')) >= 4]  # Only include sentences that have more than 4 words\n",
        "\n",
        "#     return sentences\n",
        "\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(lor):\n",
        "    lor = re.sub(r\"\\n\", \" \", lor)  # replace newline characters with space\n",
        "    lor = re.sub(r\"\\\"\", \"\", lor)  # remove quotes\n",
        "    lor = re.sub(r\"\\\\\", \"\", lor)  # remove backslashes\n",
        "    lor = re.sub(r\"\\(\", \"\", lor)  # remove open parenthesis\n",
        "    lor = re.sub(r\"\\)\", \"\", lor)  # remove close parenthesis\n",
        "    lor = re.sub(r\"Dr\\.\", \"Dr\", lor)  # handle \"Dr.\"\n",
        "    lor = re.sub(r\"\\s+\", \" \", lor)  # replace multiple spaces with a single space\n",
        "    lor = re.sub(r\"\\b(\\d+)\\.\", r\"\\1\", lor)  # remove dot after numbers, used in listing\n",
        "    return lor\n",
        "\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_https_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Example usage:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_excel('/content/drive/My Drive/app/sudee.xlsx')\n",
        "# %%\n",
        "\n",
        "# %%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "yKtwEG1I94F_",
        "outputId": "8942c6be-599c-438a-8f16-fbbfa2c5674b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8529b962-4d41-44a7-bb1e-436164791267\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8529b962-4d41-44a7-bb1e-436164791267\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving tokenizerr.py to tokenizerr.py\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload a file from your local machine to Google Colab\n",
        "upload_file = files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CJMfRZFlhCDz"
      },
      "outputs": [],
      "source": [
        "df[\"report_id\"] = df.index\n",
        "import tokenizerr as alperen\n",
        "\n",
        "df_new = alperen.split_sentences(df,\"lor1\")\n",
        "# %%\n",
        "# %%\n",
        "# Ensure we only work with rows that have all needed fields\n",
        "df = df_new.dropna(subset=[\"lor1\"])\n",
        "\n",
        "# Initialize list to hold results for each letter\n",
        "results = []\n",
        "# %%\n",
        "# df_male2male = df.loc[(df[\"gender\"]==\"male\")&(df[\"lor1_gender\"]==\"male\")]\n",
        "df = df.dropna(subset=[\"isentence\"])\n",
        "# %%\n",
        "df[\"sentences\"] = df.groupby(\"report_id\")[\"isentence\"].agg(list)\n",
        "\n",
        "df = df.dropna(subset=[\"sentences\"])\n",
        "\n",
        "# %%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70kBtT7vN8cJ"
      },
      "outputs": [],
      "source": [
        "df['sentences']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx5knMuC78_A"
      },
      "outputs": [],
      "source": [
        "sentences = df.loc[0,'sentences']\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp__SuGq8ymn"
      },
      "outputs": [],
      "source": [
        " for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence):\n",
        "            continue\n",
        "\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        print(perplexity, sentence)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pLj4vewUkH1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq7f5qMI-LiM",
        "outputId": "a55d2b79-e2bb-4ed8-a41c-a2d4c5163313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "df_male2male = df.loc[(df[\"gender\"]==\"male\")&(df[\"lor1_gender\"]==\"male\")]\n",
        "df_female2male = df.loc[(df[\"gender\"]==\"male\")&(df[\"lor1_gender\"]==\"female\")]\n",
        "df_male2female = df.loc[(df[\"gender\"]==\"female\")&(df[\"lor1_gender\"]==\"male\")]\n",
        "df_female2female = df.loc[(df[\"gender\"]==\"female\")&(df[\"lor1_gender\"]==\"female\")]\n",
        "\n",
        "m2m = []\n",
        "f2m = []\n",
        "m2f = []\n",
        "f2f = []\n",
        "\n",
        "for inx, row in df_male2male.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "for inx, row in df_male2male.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "\n",
        "    if sentence_count:\n",
        "        m2m.append(total_perplexity / sentence_count )\n",
        "\n",
        "for inx, row in df_female2male.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        f2m.append(total_perplexity / sentence_count )\n",
        "for inx, row in df_male2female.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        m2f.append(total_perplexity / sentence_count )\n",
        "for inx, row in df_female2female.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        f2f.append(total_perplexity / sentence_count )\n",
        "\n",
        "\n",
        "print('done')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoWwVHzvGM-E",
        "outputId": "6f9b4210-8423-4f09-d875-8422c71587af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_excel('/content/drive/My Drive/app/sudee.xlsx')\n",
        "df[\"report_id\"] = df.index\n",
        "import tokenizerr as alperen\n",
        "df_new = alperen.split_sentences(df, \"lor2\")\n",
        "# %%\n",
        "# %%\n",
        "# Ensure we only work with rows that have all needed fields\n",
        "df = df_new.dropna(subset=[\"lor2\"])\n",
        "\n",
        "# Initialize list to hold results for each letter\n",
        "results = []\n",
        "# %%\n",
        "# df_male2male = df.loc[(df[\"gender\"]==\"male\")&(df[\"lor1_gender\"]==\"male\")]\n",
        "df = df.dropna(subset=[\"isentence\"])\n",
        "# %%\n",
        "df[\"sentences\"] = df.groupby(\"report_id\")[\"isentence\"].agg(list)\n",
        "\n",
        "df = df.dropna(subset=[\"sentences\"])\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "df_male2male = df.loc[(df[\"gender\"]==\"male\")&(df[\"lor2_gender\"]==\"male\")]\n",
        "df_female2male = df.loc[(df[\"gender\"]==\"male\")&(df[\"lor2_gender\"]==\"female\")]\n",
        "df_male2female = df.loc[(df[\"gender\"]==\"female\")&(df[\"lor2_gender\"]==\"male\")]\n",
        "df_female2female = df.loc[(df[\"gender\"]==\"female\")&(df[\"lor2_gender\"]==\"female\")]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for inx, row in df_male2male.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        m2m.append(total_perplexity / sentence_count )\n",
        "\n",
        "for inx, row in df_female2male.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        f2m.append(total_perplexity / sentence_count )\n",
        "for inx, row in df_male2female.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        m2f.append(total_perplexity / sentence_count )\n",
        "for inx, row in df_female2female.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        f2f.append(total_perplexity / sentence_count )\n",
        "\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-TgvNwa1rKr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fyBZ2YXXIdk"
      },
      "outputs": [],
      "source": [
        "male_applicant = m2m + f2m\n",
        "male_applicant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EMl0hkNXfeb"
      },
      "outputs": [],
      "source": [
        "female_applicant = m2f + f2f\n",
        "female_applicant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2McCTt4jkU3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VknKImYUGSWZ",
        "outputId": "298139d4-412d-404e-bc78-2a8718c559a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_excel('/content/drive/My Drive/app/sudee.xlsx')\n",
        "df[\"report_id\"] = df.index\n",
        "import tokenizerr as alperen\n",
        "\n",
        "df_new = alperen.split_sentences(df, \"lor3\")\n",
        "# %%\n",
        "# %%\n",
        "# Ensure we only work with rows that have all needed fields\n",
        "df = df_new.dropna(subset=[\"lor3\"])\n",
        "\n",
        "# Initialize list to hold results for each letter\n",
        "results = []\n",
        "# %%\n",
        "# df_male2male = df.loc[(df[\"gender\"]==\"male\")&(df[\"lor1_gender\"]==\"male\")]\n",
        "df = df.dropna(subset=[\"isentence\"])\n",
        "# %%\n",
        "df[\"sentences\"] = df.groupby(\"report_id\")[\"isentence\"].agg(list)\n",
        "\n",
        "df = df.dropna(subset=[\"sentences\"])\n",
        "\n",
        "# %%\n",
        "\n",
        "df_male2male = df.loc[(df[\"gender\"]==\"male\")&(df[\"lor3_gender\"]==\"male\")]\n",
        "df_female2male = df.loc[(df[\"gender\"]==\"male\")&(df[\"lor3_gender\"]==\"female\")]\n",
        "df_male2female = df.loc[(df[\"gender\"]==\"female\")&(df[\"lor3_gender\"]==\"male\")]\n",
        "df_female2female = df.loc[(df[\"gender\"]==\"female\")&(df[\"lor3_gender\"]==\"female\")]\n",
        "\n",
        "\n",
        "for inx, row in df_male2male.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        m2m.append(total_perplexity / sentence_count )\n",
        "\n",
        "for inx, row in df_female2male.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        f2m.append(total_perplexity / sentence_count )\n",
        "for inx, row in df_male2female.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        m2f.append(total_perplexity / sentence_count )\n",
        "for inx, row in df_female2female.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    total_perplexity = 0\n",
        "    sentence_count = 0\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence) or sentence == 'none':\n",
        "            continue\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids.cuda(), labels=input_ids.cuda())\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood))\n",
        "        total_perplexity += perplexity.item()\n",
        "        sentence_count += 1\n",
        "    if sentence_count:\n",
        "        f2f.append(total_perplexity / sentence_count )\n",
        "\n",
        "print('done')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z5Sm9Ggh8eb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe6Wybkth8hu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbrSipnPh8kT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "jDoprVE-DC8g",
        "outputId": "2544d8f5-7df5-4cea-9413-cabf9e24f1bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([ 2.,  7., 13.,  6.,  7.,  7.,  4.,  8.,  8.,  5.]),\n",
              " array([24.33995962, 31.56345455, 38.78694948, 46.01044441, 53.23393934,\n",
              "        60.45743427, 67.6809292 , 74.90442412, 82.12791905, 89.35141398,\n",
              "        96.57490891]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "m = [x for x in m2m if x< 100]\n",
        "plt.hist(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GMQPVlqIl39",
        "outputId": "4b07f196-46a9-409d-c7af-0ec08a818686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MannwhitneyuResult(statistic=446.0, pvalue=0.42367958212851775)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "mannwhitneyu(m2m,f2f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUnw8jqgjxg3",
        "outputId": "e2a52085-408f-4d02-c000-a9bbd752f375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1481.1827674180656\n"
          ]
        }
      ],
      "source": [
        "import statistics\n",
        "ortalama = statistics.mean(m2m)\n",
        "\n",
        "print(ortalama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyCkV_AEjygI",
        "outputId": "5c541440-e701-4cc0-fbe7-f6b3d8e986e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1396.7485976910098\n"
          ]
        }
      ],
      "source": [
        "import statistics\n",
        "ortalama = statistics.mean(male_applicant)\n",
        "\n",
        "print(ortalama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-H6gpuSTZZC",
        "outputId": "8d5292b0-8124-43d2-8ff7-f33733aa597c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "138.62291560616887\n"
          ]
        }
      ],
      "source": [
        "import statistics\n",
        "ortalama = statistics.mean(female_applicant)\n",
        "\n",
        "print(ortalama)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V84HreGmoh6o",
        "outputId": "22856738-cee4-459b-9462-414a11996b8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MannwhitneyuResult(statistic=5983.0, pvalue=0.5178501584148261)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "mannwhitneyu(male_applicant,female_applicant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDm5civCSO3q",
        "outputId": "46a3f200-b987-4ecb-bbfe-3246a3809ea5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "422"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(f2f + m2m + m2f + f2m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZkbrafQ7t6v"
      },
      "outputs": [],
      "source": [
        "m2m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7caietJExLWk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htka8sG4iBaF",
        "outputId": "2afb0b28-368c-41d5-f238-e9164549703b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[51.83848876953125,\n",
              " 77.19868166106087,\n",
              " 71.90415734511156,\n",
              " 91.25778100967408,\n",
              " 67.89684368582333,\n",
              " 44.90344575735239,\n",
              " 47.45860164815729,\n",
              " 94.97915337880453,\n",
              " 55.71218757629394,\n",
              " 63.16116926405165,\n",
              " 106.69543139139812,\n",
              " 75.72267432439895,\n",
              " 68.39727428981236,\n",
              " 46.307885540856255,\n",
              " 53.950313091278076,\n",
              " 85.6636563709804,\n",
              " 1054.1631924510002,\n",
              " 49.58017140626907,\n",
              " 73.02157615479969,\n",
              " 72.61532627452503,\n",
              " 139.11092472076416,\n",
              " 107.22609224319459,\n",
              " 48.82726327996505,\n",
              " 930.660899480184,\n",
              " 75.59934660104605,\n",
              " 138.6820909023285,\n",
              " 186.72481759389242,\n",
              " 100.67409133911133,\n",
              " 195.1871161778768,\n",
              " 58.02142131328583,\n",
              " 104.07661407470704,\n",
              " 78.6387300491333,\n",
              " 58.69879583690477]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m2f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ge4TD3TiBdV",
        "outputId": "0f24302d-9dbb-4e83-f4ab-58b36f627d77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f2f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hasdl_LBiBlN",
        "outputId": "4bc33173-0765-42d5-f763-9ab201a18723"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[54.35071315765381,\n",
              " 25.162222461700438,\n",
              " 65.22282956994098,\n",
              " 125.30460166931152,\n",
              " 96.24267673492432,\n",
              " 58.27220262799944,\n",
              " 147.2119416323575,\n",
              " 51.367555379867554,\n",
              " 103.46158752441406,\n",
              " 31.693545409611293,\n",
              " 73.20732589562733,\n",
              " 55.665575780366595,\n",
              " 973.2687151696947,\n",
              " 72.74833229609898,\n",
              " 109.49318033106186,\n",
              " 158.14651051689597,\n",
              " 63.758888469022864,\n",
              " 64.24280637853286,\n",
              " 34.756026436300836,\n",
              " 63.350391970740425,\n",
              " 84.87053447299533,\n",
              " 73.88829771677653,\n",
              " 73.37076084430402,\n",
              " 44.43717176263983]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f2m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbzh8VB_jKNA",
        "outputId": "aa8744e9-c453-47f0-de8b-fc654e69000a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[40.018541279961084,\n",
              " 1148.6021684010823,\n",
              " 73.86677005555894,\n",
              " 48.5046323629526,\n",
              " 6114.210303159861,\n",
              " 78.18155750861534,\n",
              " 242.08780080080032,\n",
              " 382.36372203826903,\n",
              " 92.29518800973892,\n",
              " 80.15613350501427,\n",
              " 36.960033369064334,\n",
              " 107.8615580201149,\n",
              " 40.52441126505534,\n",
              " 84.17292437833898,\n",
              " 406.1264228820801,\n",
              " 63.12578565733774,\n",
              " 62.68765753507614,\n",
              " 57.026059277852376,\n",
              " 45.7401186136099,\n",
              " 62.61028775301847,\n",
              " 105.98234232734231,\n",
              " 74.26828479766846,\n",
              " 134.33716719491142,\n",
              " 121.07741864522298,\n",
              " 47.71209107912504,\n",
              " 41.613966369628905,\n",
              " 135.56150975594153,\n",
              " 38.56335115432739,\n",
              " 149.02487698307743,\n",
              " 33.29802479463465,\n",
              " 109.8145341043887,\n",
              " 80.95685214996338,\n",
              " 43.95231666564941,\n",
              " 210.64435017108917,\n",
              " 64.69980499148369,\n",
              " 328715.5257725186,\n",
              " 89.15928004338191,\n",
              " 85.49200981541684,\n",
              " 63.193345832824704,\n",
              " 41.194289207458496,\n",
              " 55.60660979327034,\n",
              " 74.96429210238986,\n",
              " 37.954018433888756,\n",
              " 100.17516475253635,\n",
              " 123.34678218239232,\n",
              " 183.40845353786762,\n",
              " 48.69132863150703,\n",
              " 48.84344930648804,\n",
              " 117.33576334847345,\n",
              " 244.32384036137506,\n",
              " 68.62155609130859,\n",
              " 167.87372620900473,\n",
              " 66.90242841508653,\n",
              " 78.89251654942831,\n",
              " 190.84906122901222,\n",
              " 92.04718898591541,\n",
              " 78.13139547620501,\n",
              " 96.5749084353447,\n",
              " 48.84220951795578,\n",
              " 161.36166515350342,\n",
              " 263.57967787981033,\n",
              " 106.70538658755166,\n",
              " 87.81235027313232,\n",
              " 59.60576793882582,\n",
              " 37.08904294967651,\n",
              " 24.339959621429443,\n",
              " 31.486665205522016,\n",
              " 95.24339840809505,\n",
              " 71.72208746885642,\n",
              " 35.031680941581726,\n",
              " 84.87841598510742,\n",
              " 46.78022368748983,\n",
              " 41.638076861699425,\n",
              " 359.79499586423236,\n",
              " 56.27553419633345,\n",
              " 59.40827129198157,\n",
              " 484.4146528977614,\n",
              " 86.25694161653519,\n",
              " 56.749253590901695,\n",
              " 872.3098828667089,\n",
              " 89.23213011423746,\n",
              " 45.34929584321522,\n",
              " 37.13047772187453,\n",
              " 76.97915625572205,\n",
              " 76.85698360866971,\n",
              " 720.1799119397214,\n",
              " 177.48196249741775,\n",
              " 162.19950138438833,\n",
              " 100.08436852914316,\n",
              " 62.18553981781006,\n",
              " 59.56129515612567,\n",
              " 40.98061840350811,\n",
              " 92.09425731658935,\n",
              " 83.00690174102783,\n",
              " 165.03268146514893,\n",
              " 42.407483196258546,\n",
              " 41.18177969559379,\n",
              " 42.22856965817903,\n",
              " 153.09598185221355,\n",
              " 213.60739088058472,\n",
              " 118.26115202903748,\n",
              " 42.3688667003925,\n",
              " 127.07457870907254,\n",
              " 59.414735317230225,\n",
              " 106.94465324401855,\n",
              " 106.7913549596613,\n",
              " 84.20425247323924,\n",
              " 74.21123847961425,\n",
              " 105.87492804527282,\n",
              " 58.83264390945435,\n",
              " 1137.2953626768929,\n",
              " 55.629163646698,\n",
              " 42.499844098091124,\n",
              " 92.38802638806794,\n",
              " 48.852984269460045,\n",
              " 78.40867787599564,\n",
              " 35337.6750149409,\n",
              " 65.52820613167502,\n",
              " 68.31185609102249,\n",
              " 171.50447966835716,\n",
              " 47.23238182067871,\n",
              " 77.32801310221355,\n",
              " 61.3382682800293,\n",
              " 126.82966300419399,\n",
              " 107.95120984857732,\n",
              " 68.4790543642911,\n",
              " 63.35366725921631,\n",
              " 103.94323786822233,\n",
              " 38.7024530342647,\n",
              " 66.99538736343384,\n",
              " 81.98493406507704,\n",
              " 63.025223027104914,\n",
              " 95.64160362879436,\n",
              " 41.35471493857248,\n",
              " 49.04568421840668,\n",
              " 13560.626316657434,\n",
              " 99.46112422943115,\n",
              " 45.06106261646046,\n",
              " 174.71190214157104,\n",
              " 110.73081023352486,\n",
              " 58.373609196056016,\n",
              " 37.7794075012207,\n",
              " 76.19500821828842,\n",
              " 69.37278053760528,\n",
              " 54.01082300004505,\n",
              " 59.2524998404763,\n",
              " 171.62382222584316,\n",
              " 70.93928106207596,\n",
              " 27.756995964050294,\n",
              " 62.34688707498404,\n",
              " 57.20360653488724,\n",
              " 67.00970228980569,\n",
              " 39.307846293729895,\n",
              " 122.84475326538086,\n",
              " 46.81850624084473,\n",
              " 95.03691596369589,\n",
              " 80.58530152638754,\n",
              " 57.09373890269887,\n",
              " 66.20859146118164,\n",
              " 42.9332321371351,\n",
              " 58190.80522531782,\n",
              " 59.556290259728065,\n",
              " 41.12065594991048,\n",
              " 116.31013911703359,\n",
              " 95.24147379738945,\n",
              " 80.48945140838623,\n",
              " 53.306028434208464,\n",
              " 77.98609892527263,\n",
              " 115.23186101232257,\n",
              " 46.91663967479359,\n",
              " 133.70466191428048,\n",
              " 39.70871680123465,\n",
              " 32.97952206929525,\n",
              " 54.42056621823992,\n",
              " 51.54663324356079,\n",
              " 397.0289478302002,\n",
              " 109.97552852630615,\n",
              " 48.696553548177086,\n",
              " 249.64207619712465,\n",
              " 62.86146774291992,\n",
              " 76.19506179369412,\n",
              " 183.11849233839246,\n",
              " 60.09062894185384,\n",
              " 127.11194200515747,\n",
              " 91.25431303183238,\n",
              " 83.18375808000565,\n",
              " 95.28811685855572,\n",
              " 83.29708545548576,\n",
              " 78.4903757373492,\n",
              " 126.14745376586914,\n",
              " 72.94275770187377,\n",
              " 80.07007209459941,\n",
              " 76.62046072218153,\n",
              " 61.813874517168315,\n",
              " 65.04077563285827,\n",
              " 55.00416946411133,\n",
              " 31.92591584523519,\n",
              " 79.50892121451241,\n",
              " 366.0412003517151,\n",
              " 71.6712574005127,\n",
              " 89.55405963550915,\n",
              " 74.12069618134271,\n",
              " 49.252152019076874,\n",
              " 50.38742143967573,\n",
              " 32.91347341952117,\n",
              " 77.97305183410644,\n",
              " 30.243039571321926,\n",
              " 104.49659865835439,\n",
              " 59.91783024714543,\n",
              " 53.123052064110254,\n",
              " 70.59863596377166,\n",
              " 54.02519642223012,\n",
              " 63.059379276476406,\n",
              " 200.43539396921793,\n",
              " 64.98117346030016,\n",
              " 32.77914905548096,\n",
              " 69.28255405426026,\n",
              " 40.524616314814644,\n",
              " 71.89800845875459,\n",
              " 96.91038066148758,\n",
              " 324.1396222114563,\n",
              " 145.21898456053302,\n",
              " 71.6217272909064,\n",
              " 48.90953572591146,\n",
              " 65.86167372597589,\n",
              " 42.811934505190166,\n",
              " 95.55622391951711,\n",
              " 52.17678654193878,\n",
              " 92.57710411071777,\n",
              " 90.7451634126551,\n",
              " 312.98151856202344,\n",
              " 81.54949017574913,\n",
              " 247.92831071217856,\n",
              " 109.80457071824507,\n",
              " 53.80245194068322,\n",
              " 114.36820440707,\n",
              " 97.06448103033978,\n",
              " 48.7248666936701,\n",
              " 141.1954526901245,\n",
              " 50.53758875528971,\n",
              " 66.23092778523763,\n",
              " 37.38070695740836,\n",
              " 75.06900934072641,\n",
              " 43.24898268959739,\n",
              " 246.49623408317566,\n",
              " 46.375090326581685,\n",
              " 53.660475730895996,\n",
              " 33.308283124651226,\n",
              " 72.73622390202114,\n",
              " 176.24075620314653,\n",
              " 248.92213207483292,\n",
              " 56.04492454528808,\n",
              " 69.20311851501465,\n",
              " 88.97836619615555,\n",
              " 772.2243391275406,\n",
              " 82.18002986907959,\n",
              " 58.60066743997427,\n",
              " 68.44064291785745,\n",
              " 42.49648230416434,\n",
              " 53.24427765607834,\n",
              " 49.01038925736039,\n",
              " 35.3950469153268,\n",
              " 32257.077180561268,\n",
              " 48.70445237840925,\n",
              " 45.73650175730388,\n",
              " 58.806127748991315,\n",
              " 52.94412002563477,\n",
              " 53.8246153317965,\n",
              " 52.84065760335615,\n",
              " 89.5967118880328,\n",
              " 43.14822483062744,\n",
              " 141.79587104103783,\n",
              " 48.962649272038384,\n",
              " 42.084213161468504,\n",
              " 156.6542529055947,\n",
              " 59.285811771046035,\n",
              " 68.16092978025738,\n",
              " 75.06840324401855,\n",
              " 79.53638264867995,\n",
              " 3550.7487893104553,\n",
              " 43.652605496920074,\n",
              " 41.093030384608674,\n",
              " 57.744533594916845,\n",
              " 110.3951211505466,\n",
              " 108.27835048328747,\n",
              " 39.512084579467775,\n",
              " 143.47901337941488,\n",
              " 98.69436422983806,\n",
              " 47.67751091718674,\n",
              " 66.00026035308838,\n",
              " 188.3431347741021,\n",
              " 90.50599352518718,\n",
              " 179.53722389539084,\n",
              " 555.9006147077007,\n",
              " 79.7900745557702,\n",
              " 79.72428173065185,\n",
              " 70.57239124510023,\n",
              " 63.95897844314575,\n",
              " 186.70010141531625,\n",
              " 62.63172796498174,\n",
              " 54.90314222971598,\n",
              " 50.51541536504572,\n",
              " 91.95370030403137,\n",
              " 50.10529539320204,\n",
              " 53.41417286612771,\n",
              " 109.25881211897907,\n",
              " 80.68511130593039,\n",
              " 8954.723306528727,\n",
              " 182.69075323985174,\n",
              " 64.39908424176667,\n",
              " 60.60571263631185,\n",
              " 69.04414664374457,\n",
              " 55.757651265462236,\n",
              " 702.2692528565725,\n",
              " 59.81630198160807,\n",
              " 128.88950729370117,\n",
              " 122.6541801535565,\n",
              " 119.48154146021062,\n",
              " 65.9192349433899,\n",
              " 50.25602575937907,\n",
              " 39.201437143179085,\n",
              " 65.31039516742413,\n",
              " 72.51060074567795,\n",
              " 63.682611306508385,\n",
              " 58.8767564535141,\n",
              " 166.44823551177979,\n",
              " 95.74980711450382,\n",
              " 103.66355260213216,\n",
              " 66.87574341717888,\n",
              " 87.61989743368966,\n",
              " 63.76237912611528,\n",
              " 81.41815365850925,\n",
              " 108.17234256267548,\n",
              " 35.19543611086332,\n",
              " 33.02456649144491,\n",
              " 72.62570510864258,\n",
              " 55.759330643547905,\n",
              " 45.336113135019936,\n",
              " 45.69925422668457,\n",
              " 196.04027039664132,\n",
              " 95.12960459391276,\n",
              " 53.43814321664664,\n",
              " 105.21212046486991,\n",
              " 69.8602803548177,\n",
              " 78.10855297420336,\n",
              " 48.604378427777974,\n",
              " 52.4585782289505,\n",
              " 82.01138718922932,\n",
              " 69.87096269925435,\n",
              " 89.4631167517768,\n",
              " 87.3941396176815,\n",
              " 129.92631875900994,\n",
              " 490.72972882877696,\n",
              " 57.672606468200684,\n",
              " 39.16572328047319,\n",
              " 77.41505025681995,\n",
              " 71.38315144181252,\n",
              " 89.31337054570515,\n",
              " 117.49213981628418,\n",
              " 75.08128592703078,\n",
              " 58.80855760574341,\n",
              " 170.6989330927531,\n",
              " 66.82885950261897,\n",
              " 16608.60835659504,\n",
              " 43.1680828332901,\n",
              " 54.35071315765381,\n",
              " 25.162222461700438,\n",
              " 65.22282956994098,\n",
              " 125.30460166931152,\n",
              " 96.24267673492432,\n",
              " 58.27220262799944,\n",
              " 147.2119416323575,\n",
              " 51.367555379867554,\n",
              " 103.46158752441406,\n",
              " 31.693545409611293,\n",
              " 73.20732589562733,\n",
              " 55.665575780366595,\n",
              " 973.2687151696947,\n",
              " 72.74833229609898,\n",
              " 109.49318033106186,\n",
              " 158.14651051689597,\n",
              " 63.758888469022864,\n",
              " 64.24280637853286,\n",
              " 34.756026436300836,\n",
              " 63.350391970740425,\n",
              " 84.87053447299533,\n",
              " 73.88829771677653,\n",
              " 73.37076084430402,\n",
              " 44.43717176263983]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "male_applicant = m2m + f2m\n",
        "male_applicant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQiMWGYki6O8",
        "outputId": "dacd74e8-86bd-4b6d-824e-1ee8d4a5f8fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "365"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(m2m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpMQxOvBjlXB",
        "outputId": "f1464f77-842e-4887-a796-dcdf2e495103"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[51.83848876953125,\n",
              " 77.19868166106087,\n",
              " 71.90415734511156,\n",
              " 91.25778100967408,\n",
              " 67.89684368582333,\n",
              " 44.90344575735239,\n",
              " 47.45860164815729,\n",
              " 94.97915337880453,\n",
              " 55.71218757629394,\n",
              " 63.16116926405165,\n",
              " 106.69543139139812,\n",
              " 75.72267432439895,\n",
              " 68.39727428981236,\n",
              " 46.307885540856255,\n",
              " 53.950313091278076,\n",
              " 85.6636563709804,\n",
              " 1054.1631924510002,\n",
              " 49.58017140626907,\n",
              " 73.02157615479969,\n",
              " 72.61532627452503,\n",
              " 139.11092472076416,\n",
              " 107.22609224319459,\n",
              " 48.82726327996505,\n",
              " 930.660899480184,\n",
              " 75.59934660104605,\n",
              " 138.6820909023285,\n",
              " 186.72481759389242,\n",
              " 100.67409133911133,\n",
              " 195.1871161778768,\n",
              " 58.02142131328583,\n",
              " 104.07661407470704,\n",
              " 78.6387300491333,\n",
              " 58.69879583690477]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "female_applicant = m2f + f2f\n",
        "female_applicant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y41J9knzxNA4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAdAzAPAxNIG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApYcgc0_xMCZ"
      },
      "outputs": [],
      "source": [
        "for inx, row in df.iterrows():\n",
        "\n",
        "    sentences = row[\"sentences\"]  # custom_sent_tokenize(paragraph)\n",
        "    max_perplexity_sentence = None\n",
        "    min_perplexity_sentence = None\n",
        "    total_perplexity = 0\n",
        "\n",
        "    # Calculate perplexity for each sentence and add to respective category\n",
        "    for sentence in sentences:\n",
        "        # Skip if sentence is NaN\n",
        "        if pd.isnull(sentence):\n",
        "            continue\n",
        "\n",
        "        input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=input_ids)\n",
        "            log_likelihood = outputs[0].item()\n",
        "\n",
        "        perplexity = torch.exp(torch.tensor(log_likelihood / input_ids.shape[-1]))\n",
        "\n",
        "        # Save every sentence and its perplexity\n",
        "        results.append(\n",
        "            {\"index\": inx, \"sentence\": sentence, \"perplexity\": perplexity.item(),}\n",
        "        )\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Add a new column 'length' which counts the number of words in each sentence\n",
        "results_df[\"length\"] = results_df[\"sentence\"].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Filter out sentences where the word count is less than 4\n",
        "results_df = results_df[results_df[\"length\"] > 4]\n",
        "\n",
        "# Write to an Excel file\n",
        "results_df.to_excel(\"son_2.xlsx\", index=False)\n",
        "\n",
        "# %%\n",
        "# sentence1 = \"Are wall food\"\n",
        "# sentence2 = \"I like reading book\"\n",
        "# sentence3 = \"I play game\"\n",
        "# input_ids = tokenizer.encode(sentence1, return_tensors=\"pt\")\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(input_ids, labels=input_ids)\n",
        "#     log_likelihood = outputs[0].item()\n",
        "\n",
        "# perplexity = torch.exp(torch.tensor(log_likelihood / input_ids.shape[-1]))\n",
        "# perplexity\n",
        "\n",
        "# %%"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
